{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1YZ5NjF2sxW9CT0GvpvaZ1SBAn-EUA7xS","timestamp":1718216100898}],"mount_file_id":"1RbRl1GO_UIRWEgfUtsZlNZ8dzsT8QwNs","authorship_tag":"ABX9TyOUx86sn1KDZ1UduRZ4MBMV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# analysis.final.b.step1\n","\n","This script loads project-specific csvs from one or more directories, adds identifying\n","information to each row marking metadata for each COBRA batch run and specific project,\n","merges with metadata about the source LNG project and destination county, and\n","calculates a few new fields for data analysis."],"metadata":{"id":"7Ud7-IY-7xcm"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"hsu5QNGLkOE6","executionInfo":{"status":"ok","timestamp":1720577184191,"user_tz":240,"elapsed":935,"user":{"displayName":"Andres Chang","userId":"01044257867117650822"}}},"outputs":[],"source":["from google.colab import drive\n","import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def load_excel_data(file_path, sheet_name=None):\n","    \"\"\"\n","    Load data from an Excel spreadsheet using Pandas.\n","\n","    Parameters:\n","    - file_path (str): The file path to the Excel spreadsheet.\n","    - sheet_name (str or int, default None): Name or index of the sheet to read. If None, it reads the first sheet.\n","\n","    Returns:\n","    - DataFrame: A pandas DataFrame containing the data.\n","    \"\"\"\n","    try:\n","        # Load data from Excel file\n","        if sheet_name is not None:\n","            data = pd.read_excel(file_path, sheet_name=sheet_name)\n","        else:\n","            data = pd.read_excel(file_path)\n","        return data\n","    except FileNotFoundError:\n","        print(\"File not found. Please provide a valid file path.\")\n","        return None\n","    except Exception as e:\n","        print(\"An error occurred:\", e)\n","        return None\n","\n","def aggregate_csvs_prj(folder_path):\n","  from warnings import simplefilter\n","  simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n","  dataframes = []\n","  # Iterate over files in the folder\n","  for filename in os.listdir(folder_path):\n","      if filename.startswith(\"results\") and filename.endswith(\".csv\"):\n","          file_path = os.path.join(folder_path, filename)\n","          ID = filename.split(\".\")[4]  # Extract name piece from filename\n","          # Read CSV file into a dataframe\n","          df = pd.read_csv(file_path, skiprows = [1], skipfooter=1, engine='python')\n","          df.loc[:, \"ID\"] = ID\n","          # Append dataframe to the list\n","          dataframes.append(df.iloc[:])\n","\n","  # Concatenate dataframes into a single grouped dataframe\n","  all_results = pd.concat(dataframes)\n","\n","  return all_results\n","\n","\n","def load_dirs(input_dirs, dir_meta_cols, FIPS_crosswalk_file):\n","    \"\"\"\n","    Load csvs from input directories and merge dataframes.\n","\n","    Parameters:\n","    - input_dirs (list): List of input directories.\n","    - dir_meta_cols (list): List of dictionaries containing metadata columns for each directory.\n","    - FIPS_crosswalk_file (str): Path to the crosswalk file containing FIPS and SOURCEINDX columns.\n","\n","    Returns:\n","    - agg_df0 (DataFrame): Merged DataFrame with added \"Destination FIPS\" column.\n","    \"\"\"\n","\n","    agg_dfs = []\n","\n","    for i in np.arange(len(input_dirs)):\n","        dfi = aggregate_csvs_prj(input_dirs[i])\n","        for key, val in dir_meta_cols[i].items():\n","            dfi[key] = val\n","        agg_dfs.append(dfi)\n","\n","    agg_df0 = pd.concat(agg_dfs)\n","    agg_df0['ID'] = agg_df0['ID'].str.replace(\"_\", \" \")\n","\n","    return agg_df0\n","\n","def load_summarized_demographic_data(path_dict, summary_col):\n","  \"\"\"\n","  Load csvs based on fnames in path_dict, calculate the total values for each field\n","  regardless of age, and drop age columns\n","  \"\"\"\n","  AgeCols = ['Age0'] + ['Age'+str(i) for i in np.arange(1, 100)]\n","\n","  out_dfs = []\n","\n","  for i in np.arange(len(path_dict['fname'])):\n","    out_df0 = pd.read_csv(path_dict['fname'][i])\n","    out_df0[summary_col] = out_df0[AgeCols].sum(axis=1)\n","    out_df0['Baseline Year'] = path_dict['year'][i]\n","    out_df0.drop(columns=AgeCols, inplace=True)\n","    out_dfs.append(out_df0)\n","\n","  return pd.concat(out_dfs)\n","\n","\n","incidence_vars = [\n","    'Total Mortality(low estimate)',\n","    'Total Mortality(high estimate)',\n","    'PM Mortality, All Cause (low)',\n","    'PM Mortality, All Cause (high)',\n","    'PM Infant Mortality',\n","    'Total O3 Mortality',\n","    'O3 Mortality (Short-term exposure)',\n","    'O3 Mortality (Long-term exposure)',\n","    'Total Asthma Symptoms',\n","    'PM Asthma Symptoms, Albuterol use',\n","    'O3 Asthma Symptoms, Chest Tightness',\n","    'O3 Asthma Symptoms, Cough',\n","    'O3 Asthma Symptoms, Shortness of Breath',\n","    'O3 Asthma Symptoms, Wheeze',\n","    'Total Incidence, Asthma',\n","    'PM Incidence, Asthma',\n","    'O3 Incidence, Asthma',\n","    'Total Incidence, Hay Fever/Rhinitis',\n","    'PM Incidence, Hay Fever/Rhinitis',\n","    'O3 Incidence, Hay Fever/Rhinitis',\n","    'Total ER Visits, Respiratory',\n","    'PM ER Visits, Respiratory',\n","    'O3 ER Visits, Respiratory',\n","    'Total Hospital Admits, All Respiratory',\n","    'PM Hospital Admits, All Respiratory',\n","    'O3 Hospital Admits, All Respiratory',\n","    'PM Nonfatal Heart Attacks',\n","    'PM Minor Restricted Activity Days',\n","    'PM Work Loss Days',\n","    'PM Incidence Lung Cancer',\n","    'PM HA Cardio Cerebro and Peripheral Vascular Disease',\n","    'PM HA Alzheimers Disease',\n","    'PM HA Parkinsons Disease',\n","    'PM Incidence Stroke',\n","    'PM Incidence Out of Hospital Cardiac Arrest',\n","    'PM ER visits All Cardiac Outcomes',\n","    'O3 ER Visits, Asthma',\n","    'O3 School Loss Days, All Cause'\n","]\n"]},{"cell_type":"code","source":["\"\"\"\n","This script loads project-specific csvs from one or more directories, adds identifying\n","information to each row marking metadata for each COBRA batch run and specific project,\n","merges with metadata about the source LNG project and destination county, and\n","calculates a few new fields for data analysis.\n","\n","\"\"\"\n","\n","# Method parameters ===========================================================\n","\n","# Batch-specific metadata to add to each row\n","dir_meta_cols = [{\"Analysis Year\": 2023, \"discount rate\": 2, \"config_id\": \"b.finalData.01\"},\n","                 {\"Analysis Year\": 2030, \"discount rate\": 2, \"config_id\": \"b.finalData.02\"},\n","                 {\"Analysis Year\": 2050, \"discount rate\": 2, \"config_id\": \"b.finalData.03\"}\n","                 ]\n","# Project-specific metadata to add to each row\n","prj_meta_cols = [\"Project\", \"Terminal\", \"Project Status\", \"DOE NFTA Authorization Status\"]\n","\n","# Manually assigned directories and file paths ================================\n","# Project data spreadsheet\n","f0 = '/content/drive/MyDrive/gpDept-ResearchDept/LNG Air Pollution/LNG Health - COBRA project/Version 5 analysis/240610-FinalData-AllProjects.xlsx'\n","\n","# Metadata spreadsheet\n","f1 = '/content/drive/MyDrive/gpDept-ResearchDept/LNG Air Pollution/LNG Health - COBRA project/COBRA_LNGHEALTH_Data.xlsx'\n","\n","# Input spreadsheets and directories\n","results_dir0 = \"/content/drive/MyDrive/gpDept-ResearchDept/LNG Air Pollution/LNG Health - COBRA project/Version 5 analysis\"\n","pop_f0 = \"/content/drive/MyDrive/gpDept-ResearchDept/LNG Air Pollution/LNG Health - COBRA project/COBRA (from desktop version) - v5.1/default data/default_YYYY_population_data.csv\"\n","\n","# FIPS crosswalk\n","FIPS_crosswalk_file = \"/content/drive/MyDrive/gpDept-ResearchDept/LNG Air Pollution/LNG Health - COBRA project/COBRA (from desktop version) - v5.1/data dictionary/SOURCEINDX to FIPS crosswalk.csv\"\n","\n","# Script ======================================================================\n","\n","# 0. Automatically assign some fpaths and directories =========================\n","pop_file_info = {'fname': [pop_f0.replace(\"YYYY\", str(i['Analysis Year'])) for i in dir_meta_cols],\n","            'year': [i['Analysis Year'] for i in dir_meta_cols]}\n","input_dirs = [(results_dir0 + '/' + i[\"config_id\"]) for i in dir_meta_cols]\n","\n","# 1. Aggregate CSVs ===========================================================\n","agg_df0 = load_dirs(input_dirs, dir_meta_cols, FIPS_crosswalk_file)\n","\n","# 2.1 Merge with project-level metadata =======================================\n","# Load project-level metadata\n","prj_meta = load_excel_data(f1, sheet_name = 'LNG Project Data')[prj_meta_cols]\n","\n","# Merge based on ID and shared SC Project Title\n","agg_df1 = agg_df0.merge(prj_meta, left_on=\"ID\", right_on =\"Project\")\n","agg_df1.drop(['ID'], axis=1, inplace=True)\n","\n","# 2.2 Merge with population and incidence data ================================\n","pop_data = load_summarized_demographic_data(pop_file_info, 'Total Population')\n","\n","agg_df2 = (agg_df1\n","           .merge(\n","               (pop_data[['FIPS', 'Baseline Year', 'Total Population']]),\n","               left_on = ['FIPS', 'Analysis Year'],\n","               right_on = ['FIPS', 'Baseline Year']\n","))\n","\n","agg_df2.rename(columns = {'County': 'Destination County', 'State': 'Destination State', 'FIPS': 'Destination FIPS'}, inplace=True)\n","\n","# 3. Calculate cases per million people\n","agg_df3 = agg_df2\n","for var in incidence_vars:\n","    new_col_name = var + ' PER MILLION'\n","    agg_df3[new_col_name] = agg_df3[var] / (agg_df3['Total Population'] / 1000000)\n","\n","# This is helpful for plotting later\n","agg_df3['Project Status'] = pd.Categorical(agg_df3['Project Status'], categories=['Operating', 'Under Construction', 'Planned'])"],"metadata":{"id":"5uUtuaslINvA","executionInfo":{"status":"ok","timestamp":1720577282810,"user_tz":240,"elapsed":97885,"user":{"displayName":"Andres Chang","userId":"01044257867117650822"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Export combined results to CSV\n","\"\"\"\n","agg_df_f0 = results_dir0 + \"/b.finalData.results/b.finalData.01-03.combined_results.csv\"\n","agg_df3.to_csv(agg_df_f0, index=False)"],"metadata":{"id":"RAv-LsvsA0Fw","executionInfo":{"status":"ok","timestamp":1720577376561,"user_tz":240,"elapsed":93753,"user":{"displayName":"Andres Chang","userId":"01044257867117650822"}}},"execution_count":3,"outputs":[]}]}