{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1PLfG6m7GvKAtT6gF63snhcfJBoxT4XDe","authorship_tag":"ABX9TyP4e3ReMowNRiHuxU1/au/n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ZkbVPBeL9X3v","executionInfo":{"status":"ok","timestamp":1723164713815,"user_tz":240,"elapsed":993,"user":{"displayName":"Andres Chang","userId":"01044257867117650822"}}},"outputs":[],"source":["from google.colab import drive\n","import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def load_excel_data(file_path, sheet_name=None):\n","    \"\"\"\n","    Load data from an Excel spreadsheet using Pandas.\n","\n","    Parameters:\n","    - file_path (str): The file path to the Excel spreadsheet.\n","    - sheet_name (str or int, default None): Name or index of the sheet to read. If None, it reads the first sheet.\n","\n","    Returns:\n","    - DataFrame: A pandas DataFrame containing the data.\n","    \"\"\"\n","    try:\n","        # Load data from Excel file\n","        if sheet_name is not None:\n","            data = pd.read_excel(file_path, sheet_name=sheet_name)\n","        else:\n","            data = pd.read_excel(file_path)\n","        return data\n","    except FileNotFoundError:\n","        print(\"File not found. Please provide a valid file path.\")\n","        return None\n","    except Exception as e:\n","        print(\"An error occurred:\", e)\n","        return None\n","\n","def aggregate_csvs_prj(folder_path):\n","  from warnings import simplefilter\n","  simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n","  dataframes = []\n","  # Iterate over files in the folder\n","  for filename in os.listdir(folder_path):\n","      if filename.startswith(\"results\") and filename.endswith(\".csv\"):\n","          file_path = os.path.join(folder_path, filename)\n","          ID = filename.split(\".\")[4]  # Extract name piece from filename\n","          # Read CSV file into a dataframe\n","          df = pd.read_csv(file_path, skiprows = [1], skipfooter=1, engine='python')\n","          df.loc[:, \"ID\"] = ID\n","          # Append dataframe to the list\n","          dataframes.append(df.iloc[:])\n","\n","  # Concatenate dataframes into a single grouped dataframe\n","  all_results = pd.concat(dataframes)\n","\n","  return all_results\n","\n","\n","def load_dirs(input_dirs, dir_meta_cols, FIPS_crosswalk_file):\n","    \"\"\"\n","    Load csvs from input directories and merge dataframes.\n","\n","    Parameters:\n","    - input_dirs (list): List of input directories.\n","    - dir_meta_cols (list): List of dictionaries containing metadata columns for each directory.\n","    - FIPS_crosswalk_file (str): Path to the crosswalk file containing FIPS and SOURCEINDX columns.\n","\n","    Returns:\n","    - agg_df0 (DataFrame): Merged DataFrame with added \"Destination FIPS\" column.\n","    \"\"\"\n","\n","    agg_dfs = []\n","\n","    for i in np.arange(len(input_dirs)):\n","        dfi = aggregate_csvs_prj(input_dirs[i])\n","        for key, val in dir_meta_cols[i].items():\n","            dfi[key] = val\n","        agg_dfs.append(dfi)\n","\n","    agg_df0 = pd.concat(agg_dfs)\n","    agg_df0['ID'] = agg_df0['ID'].str.replace(\"_\", \" \")\n","\n","    return agg_df0\n","\n","def load_summarized_demographic_data(path_dict, summary_col):\n","  \"\"\"\n","  Load csvs based on fnames in path_dict, calculate the total values for each field\n","  regardless of age, and drop age columns\n","  \"\"\"\n","  AgeCols = ['Age0'] + ['Age'+str(i) for i in np.arange(1, 100)]\n","\n","  out_dfs = []\n","\n","  for i in np.arange(len(path_dict['fname'])):\n","    out_df0 = pd.read_csv(path_dict['fname'][i])\n","    out_df0[summary_col] = out_df0[AgeCols].sum(axis=1)\n","    out_df0['Baseline Year'] = path_dict['year'][i]\n","    out_df0.drop(columns=AgeCols, inplace=True)\n","    out_dfs.append(out_df0)\n","\n","  return pd.concat(out_dfs)\n","\n","\n","incidence_vars = [\n","    'Total Mortality(low estimate)',\n","    'Total Mortality(high estimate)',\n","    'PM Mortality, All Cause (low)',\n","    'PM Mortality, All Cause (high)',\n","    'PM Infant Mortality',\n","    'Total O3 Mortality',\n","    'O3 Mortality (Short-term exposure)',\n","    'O3 Mortality (Long-term exposure)',\n","    'Total Asthma Symptoms',\n","    'PM Asthma Symptoms, Albuterol use',\n","    'O3 Asthma Symptoms, Chest Tightness',\n","    'O3 Asthma Symptoms, Cough',\n","    'O3 Asthma Symptoms, Shortness of Breath',\n","    'O3 Asthma Symptoms, Wheeze',\n","    'Total Incidence, Asthma',\n","    'PM Incidence, Asthma',\n","    'O3 Incidence, Asthma',\n","    'Total Incidence, Hay Fever/Rhinitis',\n","    'PM Incidence, Hay Fever/Rhinitis',\n","    'O3 Incidence, Hay Fever/Rhinitis',\n","    'Total ER Visits, Respiratory',\n","    'PM ER Visits, Respiratory',\n","    'O3 ER Visits, Respiratory',\n","    'Total Hospital Admits, All Respiratory',\n","    'PM Hospital Admits, All Respiratory',\n","    'O3 Hospital Admits, All Respiratory',\n","    'PM Nonfatal Heart Attacks',\n","    'PM Minor Restricted Activity Days',\n","    'PM Work Loss Days',\n","    'PM Incidence Lung Cancer',\n","    'PM HA Cardio Cerebro and Peripheral Vascular Disease',\n","    'PM HA Alzheimers Disease',\n","    'PM HA Parkinsons Disease',\n","    'PM Incidence Stroke',\n","    'PM Incidence Out of Hospital Cardiac Arrest',\n","    'PM ER visits All Cardiac Outcomes',\n","    'O3 ER Visits, Asthma',\n","    'O3 School Loss Days, All Cause'\n","]\n"]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive') # Comment this out if running ipynb locally\n","wdir = '/content/drive/MyDrive/gpDept-ResearchDept/LNG Air Pollution/LNG Health - COBRA project/git_repo/Permit-To-Kill-COBRA-Research/' # Replace this with your working directory path\n","os.chdir(wdir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPKqtSx_awZq","executionInfo":{"status":"ok","timestamp":1723164741713,"user_tz":240,"elapsed":27900,"user":{"displayName":"Andres Chang","userId":"01044257867117650822"}},"outputId":"e5e92688-d505-41d1-8cba-875b1995a13b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\n","# Method parameters ===========================================================\n","\n","# Batch-specific metadata to add to each row\n","dir_meta_cols = [{\"discount rate\": 2, \"config_id\": \"a.finalData.01\"},\n","                 {\"discount rate\": 2, \"config_id\": \"a.finalData.02\"},\n","                 {\"discount rate\": 2, \"config_id\": \"a.finalData.03\"}\n","                 ]\n","# Project-specific metadata to add to each row\n","prj_meta_cols = [\"Project\", \"Terminal\", \"Project Status\", \"DOE NFTA Authorization Status\"]\n","\n","# Manually assigned directories and file paths ================================\n","# Project data spreadsheet\n","f0 = 'Version 5 analysis/240610-FinalData-AllProjects.xlsx'\n","\n","# Input spreadsheets and directories\n","results_dir0 = \"Version 5 analysis\"\n","\n","# FIPS crosswalk\n","FIPS_crosswalk_file = \"COBRA (from desktop version) - v5.1/data dictionary/SOURCEINDX to FIPS crosswalk.csv\"\n","\n","# Script ======================================================================\n","input_dirs = [(results_dir0 + '/' + i[\"config_id\"]) for i in dir_meta_cols]\n","\n","# 1. Aggregate CSVs ===========================================================\n","agg_df0 = load_dirs(input_dirs, dir_meta_cols, FIPS_crosswalk_file)\n","agg_df0.sort_values(by=['config_id', 'ID'], inplace=True)"],"metadata":{"id":"oZ4M4nx1liUI","executionInfo":{"status":"ok","timestamp":1723164812252,"user_tz":240,"elapsed":70545,"user":{"displayName":"Andres Chang","userId":"01044257867117650822"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 2. Add discounted $ columns (from a 2023 baseline) ==========================\n","dol_cols = [i for i in agg_df0.columns if \"$\" in i]\n","d = 0.02\n","\n","for col in [dol_cols[i] for i in [0, 1, 2, 3, 10, 16]]:\n","    # Create a new column name for the discounted values\n","    discounted_col = f'{col} DISCOUNTED'\n","\n","    # Calculate the discounted value using the formula\n","    agg_df0[discounted_col] = agg_df0.apply(\n","        lambda row: row[col] * (1 / (1 + d) ** (int(row['ID']) - 2023)),\n","        axis=1\n","    )"],"metadata":{"id":"jpnUE4pIcYXr","executionInfo":{"status":"ok","timestamp":1723164839366,"user_tz":240,"elapsed":27121,"user":{"displayName":"Andres Chang","userId":"01044257867117650822"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 3. Save combined dataframe as a CSV\n","\n","agg_df_f0 = results_dir0 + \"/a.finalData.results/a.finalData.01-03.combined_results.csv\"\n","agg_df0.to_csv(agg_df_f0, index=False)"],"metadata":{"id":"zXULvHCHYU52","executionInfo":{"status":"ok","timestamp":1723164890749,"user_tz":240,"elapsed":51386,"user":{"displayName":"Andres Chang","userId":"01044257867117650822"}}},"execution_count":5,"outputs":[]}]}